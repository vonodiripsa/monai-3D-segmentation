{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Brain tumor 3D segmentation with AzureML and MONAI (BRATS21)\n",
        "\n",
        "Glioma brain tumors are among the most aggressive and lethal types of brain tumors. They can cause a range of symptoms, including headaches, seizures, and difficulty with speech and movement. Gliomas can be difficult to diagnose and treat, and early detection is critical for improving patient outcomes.\n",
        "\n",
        "Computer vision AI has emerged as a promising tool for supporting the diagnosis and treatment of glioma brain tumors. AI algorithms can analyze medical images of the brain and identify the location and extent of tumors with a high degree of accuracy. This can help clinicians make more informed decisions about treatment options, such as surgery or radiation therapy, and monitor the progress of the disease over time. Additionally, AI algorithms can help researchers better understand the underlying biology of gliomas and develop new therapies for this challenging disease.\n",
        "\n",
        "This demo is based on the [MONAI 3d brain tumor segmentation tutorial](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/swin_unetr_brats21_segmentation_3d.ipynb) and shows how to construct a training workflow of multi-labels segmentation task.\n",
        "\n",
        "The sub-regions considered for evaluation in the BraTS 21 challenge are the \"enhancing tumor\" (ET), the \"tumor core\" (TC), and the \"whole tumor\" (WT). The ET is described by areas that show hyper-intensity in T1Gd when compared to T1, but also when compared to “healthy” white matter in T1Gd. The TC describes the bulk of the tumor, which is what is typically resected. The TC entails the ET, as well as the necrotic (NCR) parts of the tumor. The appearance of NCR is typically hypo-intense in T1-Gd when compared to T1. The WT describes the complete extent of the disease, as it entails the TC and the peritumoral edematous/invaded tissue (ED), which is typically depicted by the hyper-intense signal in FLAIR [BraTS 21].\n",
        "\n",
        "![image](./media/fig_brats21.png)\n",
        "\n",
        "This notebook has been developed and tested with VSCode connected to an AzureML `STANDARD_D13_V2` Compute Instance using the `azureml_py310_sdkv2` kernel.\n",
        "\n",
        "## References\n",
        "\n",
        "[1]: Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H. and Xu, D., 2022. Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images. arXiv preprint arXiv:2201.01266.\n",
        "\n",
        "[2]: Tang, Y., Yang, D., Li, W., Roth, H.R., Landman, B., Xu, D., Nath, V. and Hatamizadeh, A., 2022. Self-supervised pre-training of swin transformers for 3d medical image analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 20730-20740).\n",
        "\n",
        "[3] U.Baid, et al., The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification, arXiv:2107.02314, 2021.\n",
        "\n",
        "[4] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, et al. \"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)\", IEEE Transactions on Medical Imaging 34(10), 1993-2024 (2015) DOI: 10.1109/TMI.2014.2377694\n",
        "\n",
        "[5] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, et al., \"Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features\", Nature Scientific Data, 4:170117 (2017) DOI: 10.1038/sdata.2017.117\n",
        "\n",
        "[6] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., \"Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-GBM collection\", The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.KLXWJJ1Q\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs and Imports\r\n",
        "Do it only once"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# based on azureml_py310_sdkv2 kernel\r\n",
        "# %pip install torch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0\r\n",
        "# %pip install 'monai[nibabel, ignite, tqdm]'\r\n",
        "# %pip install itkwidgets"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680762655603
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import tempfile\r\n",
        "import base64\r\n",
        "import json\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import torch\r\n",
        "\r\n",
        "import tarfile\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "from itkwidgets import view\r\n",
        "from ipywidgets import interact\r\n",
        "\r\n",
        "from azure.identity import DefaultAzureCredential\r\n",
        "from azure.ai.ml import MLClient, command, Input, dsl\r\n",
        "from azure.ai.ml.constants import AssetTypes\r\n",
        "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, Model, Environment, JobService, Data, CodeConfiguration, OnlineRequestSettings, AmlCompute\r\n",
        "from azure.core.exceptions import ResourceNotFoundError\r\n",
        "\r\n",
        "from monai.apps import DecathlonDataset\r\n",
        "from monai.data import DataLoader, Dataset\r\n",
        "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, EnsureTyped, Orientationd, Spacingd, NormalizeIntensityd, MapTransform\r\n",
        "from monai.visualize.utils import blend_images\r\n",
        "from azure.ai.ml import load_component\r\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680762675719
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Define central variables"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "experiment_name = 'monai-brain-tumor-segmentation' # AzureML experiment name\n",
        "#dataset_name=\"BRATS-dataset\"\n",
        "train_target = 'NC64asT4v3-GPU'\n",
        "\n",
        "# Model Deployment\n",
        "registered_model_name = 'SegResNet'\n",
        "# deployment_name = 'blue'\n",
        "\n",
        "#Environments\n",
        "train_env_name = \"monai-demo-train-env\"\n",
        "\n",
        "#Pipeline\n",
        "pipeline_name = 'training_pipeline'\n",
        "\n",
        "#BraTS data from kaggle\n",
        "tar_location = 'azureml://subscriptions/b7d41fc8-d35d-41db-92ed-1f7f1d32d4d9/resourcegroups/monai-3d-rg/workspaces/aml-monai-3d/datastores/data/paths/braintumor/BraTS2021_Training_Data.tar'\n",
        "\n",
        "# Visualization and validation sample\n",
        "sample_image = '../samples/BraTS2021_00402_flair.nii.gz' \n",
        "sample_image_t1 = '../samples/BraTS2021_00402_t1.nii.gz'\n",
        "sample_image_t1ce = '../samples/BraTS2021_00402_t1ce.nii.gz'\n",
        "sample_image_t2 = '../samples/BraTS2021_00402_t2.nii.gz'\n",
        "sample_label = '../samples/BraTS2021_00402_seg.nii.gz'"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1680762675875
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\r\n",
        "    credential = DefaultAzureCredential()\r\n",
        "    # Check if given credential can get token successfully.\r\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\r\n",
        "except Exception as ex:\r\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\r\n",
        "    # This will open a browser page for\r\n",
        "    credential = InteractiveBrowserCredential()\r\n",
        "\r\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-04-06 06:31:14,848 - No environment configuration found.\n2023-04-06 06:31:14,849 - ManagedIdentityCredential will use Azure ML managed identity\n2023-04-06 06:31:14,957 - DefaultAzureCredential acquired a token from ManagedIdentityCredential\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680762676047
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Inspect sample image"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\r\n",
        "    \"\"\"\r\n",
        "    Convert labels to multi channels based on brats 2021 classes:\r\n",
        "    label 1 necrotic tumor core (NCR)\r\n",
        "    label 2 peritumoral edematous/invaded tissue \r\n",
        "    label 3 is not used in the new dataset version\r\n",
        "    label 4 GD-enhancing tumor \r\n",
        "    The possible classes are:\r\n",
        "      TC (Tumor core): merge labels 1 and 4\r\n",
        "      WT (Whole tumor): merge labels 1,2 and 4\r\n",
        "      ET (Enhancing tumor): label 4\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __call__(self, data):\r\n",
        "        d = dict(data)\r\n",
        "        for key in self.keys:\r\n",
        "            result = []\r\n",
        "            # merge label 1 and label 4 to construct TC\r\n",
        "            result.append(torch.logical_or(d[key] == 1, d[key] == 4))\r\n",
        "            # merge labels 1, 2 and 4 to construct WT\r\n",
        "            result.append(\r\n",
        "                torch.logical_or(\r\n",
        "                    torch.logical_or(d[key] == 1, d[key] == 2), d[key] == 4\r\n",
        "                )\r\n",
        "            )\r\n",
        "            # label 4 is ET\r\n",
        "            result.append(d[key] == 4)\r\n",
        "            d[key] = torch.stack(result, axis=0).float()\r\n",
        "        return d\r\n",
        "\r\n",
        "val_transform = Compose(\r\n",
        "[\r\n",
        "    LoadImaged(keys=[\"image\", \"label\"]),\r\n",
        "    EnsureChannelFirstd(keys=\"image\"),\r\n",
        "    EnsureTyped(keys=[\"image\", \"label\"]),\r\n",
        "    ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\r\n",
        "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\r\n",
        "    Spacingd(\r\n",
        "        keys=[\"image\", \"label\"],\r\n",
        "        pixdim=(1.0, 1.0, 1.0),\r\n",
        "        mode=(\"bilinear\", \"nearest\"),\r\n",
        "    ),\r\n",
        "    NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\r\n",
        "])\r\n",
        "\r\n",
        "data_list = [{'image': sample_image, 'label': sample_label}]\r\n",
        "val_ds = Dataset(data=data_list, transform=val_transform)\r\n",
        "\r\n",
        "img_vol = val_ds[0][\"image\"].numpy()\r\n",
        "seg_vol = val_ds[0][\"label\"].numpy()\r\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680762677125
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect 3d structure - viewer only works in VSCode \r\n",
        "\r\n",
        "# img_vol_ch = img_vol[0,:,:,:]\r\n",
        "# seg_vol_ch = seg_vol[0,:,:,:]\r\n",
        "\r\n",
        "\r\n",
        "# viewer = view(image= img_vol_ch * 255,\r\n",
        "#               label_image= seg_vol_ch * 255,\r\n",
        "#               gradient_opacity=0.4,\r\n",
        "#               background = (0.5, 0.5, 0.5))\r\n",
        "              \r\n",
        "# viewer"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680762677357
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# viewer.close()"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680762677597
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Create compute resources, environments and datasets\n",
        "__Note:__ Creating compute resources, training/scoring environments and the dataset __need only performed once__. If you have executed these steps previously, navigate to the next section of this notebook.  \n",
        "\n",
        "Note that we are using low priority compute in this demo as the most cost efficient option. Low priority VMs are significantly cheaper than standard dedictaed compute. However, these resources are not always available and there is a risk that a training job might be pre-empted."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    _ = ml_client.compute.get(train_target)\n",
        "    print(\"Found existing compute target.\")\n",
        "except ResourceNotFoundError:\n",
        "    print(\"Creating a new compute target...\")\n",
        "    compute_config = AmlCompute(\n",
        "        name=train_target,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"STANDARD_NC24RS_V3\", # 4 x Tesla V100, 16 GB GPU memory each\n",
        "        tier=\"low_priority\",\n",
        "        idle_time_before_scale_down=600,\n",
        "        min_instances=0,\n",
        "        max_instances=2,\n",
        "    )\n",
        "    ml_client.begin_create_or_update(compute_config)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-04-06 06:31:16,284 - DefaultAzureCredential acquired a token from ManagedIdentityCredential\nFound existing compute target.\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1680762677900
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Get training environment\n",
        "training_environment = ml_client.environments.get(name=train_env_name,label='latest')\n",
        "\n",
        "#Create training environment\n",
        "#Run it once\n",
        "# training_environment = Environment(\n",
        "#     image=\"mcr.microsoft.com/azureml/\" + \"openmpi4.1.0-cuda11.1-cudnn8-ubuntu20.04:latest\",\n",
        "#     conda_file=\"./train-env.yml\",\n",
        "#     name=train_env,\n",
        "#     description=\"Parallel PyTorch training on AzureML with MONAI\")\n",
        "\n",
        "# ml_client.environments.create_or_update(training_environment)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-04-06 06:31:17,046 - DefaultAzureCredential acquired a token from ManagedIdentityCredential\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1680762678242
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from azure.ai.ml import load_component\r\n",
        "# importing the Component Package\r\n",
        "from azure.ai.ml import load_component\r\n",
        "upload_component = load_component(source=\"../components/upload_from_blob/spec.yaml\")\r\n",
        "from azure.ai.ml import load_component\r\n",
        "train_component = load_component(source=\"../components/train_segmentation/spec.yaml\")"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680762678454
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.pipeline(\r\n",
        "    name=pipeline_name,\r\n",
        "    description=f'Pipeline for segmentation. The unique identifier is that can help you to track files in the storage account.',\r\n",
        "    default_compute=train_target,\r\n",
        ")\r\n",
        "def image_pipeline_func(pipeline_input_file):\r\n",
        "\r\n",
        "    # Load data pipeline step   \r\n",
        "    load_step = upload_component(\r\n",
        "        blob_file_location=pipeline_input_file,\r\n",
        "    )\r\n",
        "    #load_step.compute = \"cpucluster\"\r\n",
        "    \r\n",
        "    # # Train pipeline step\r\n",
        "    train_step = train_component(\r\n",
        "        input_data=load_step.outputs.image_data_folder, best_model_name=registered_model_name\r\n",
        "    )\r\n",
        "    train_step.distribution.process_count_per_instance=4\r\n",
        "    train_step.resources = {'instance_count' : 1, 'shm_size':'300g'}\r\n",
        "    train_step.environment_variables = {'AZUREML_ARTIFACTS_DEFAULT_TIMEOUT' : '1000'}\r\n",
        "\r\n",
        "    return {\r\n",
        "        \"model\" : train_step.outputs.model,\r\n",
        "    }\r\n"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680762678612
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_job = image_pipeline_func(\r\n",
        "       pipeline_input_file=Input(type=\"uri_file\", path=tar_location),\r\n",
        ")\r\n",
        "\r\n",
        "# pipeline_job.outputs.uncompressed_data = Output(type=\"uri_folder\", path=\"...\")\r\n",
        "# pipeline_job.outputs.model = Output(type=\"uri_folder\", path=\"...\")\r\n",
        "\r\n",
        "ml_client.jobs.create_or_update(pipeline_job, experiment_name=experiment_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-04-06 06:31:23,800 - DefaultAzureCredential acquired a token from ManagedIdentityCredential\n2023-04-06 06:31:26,498 - DefaultAzureCredential acquired a token from ManagedIdentityCredential\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "PipelineJob({'inputs': {'pipeline_input_file': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f5f62fffa00>}, 'outputs': {'model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f5f62ffffd0>}, 'jobs': {}, 'component': PipelineComponent({'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': 'Pipeline for segmentation. The unique identifier is that can help you to track files in the storage account.', 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f5f62ffed40>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'image_pipeline', 'is_deterministic': None, 'inputs': {'pipeline_input_file': {}}, 'outputs': {'model': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'load_step': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'load_step', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f5f62e5e0e0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (INFO)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'blob_file_location': '${{parent.inputs.pipeline_input_file}}'}, 'job_outputs': {}, 'inputs': {'blob_file_location': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f5f62e5f9a0>}, 'outputs': {}, 'component': 'azureml_anonymous:82ddf905-5893-4721-b8eb-35c1b4306c9d', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '35713023-b230-4485-a47a-1ca23c6d5ddf', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False}), 'train_step': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'train_step', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f5f62e5e8f0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (INFO)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'best_model_name': 'SegResNet', 'input_data': '${{parent.jobs.load_step.outputs.image_data_folder}}'}, 'job_outputs': {'model': '${{parent.outputs.model}}'}, 'inputs': {'best_model_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f5f62e5d090>, 'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f5f62e5ed70>}, 'outputs': {'model': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f5f62e5e470>}, 'component': 'azureml_anonymous:37aa94c1-ae7a-4b70-90d7-767f0d6cd331', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'fac0b9e4-66b3-44b9-9bdb-9f93a2637d74', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': <azure.ai.ml.entities._job.distribution.PyTorchDistribution object at 0x7f5f642baa10>, 'environment_variables': {'AZUREML_ARTIFACTS_DEFAULT_TIMEOUT': '1000'}, 'environment': None, 'resources': {'instance_count': 1, 'shm_size': '300g'}, 'swept': False})}, 'job_types': {'command': 2}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'sharp_office_l4h4r612dg', 'description': 'Pipeline for segmentation. The unique identifier is that can help you to track files in the storage account.', 'tags': {}, 'properties': {'azureml.DevPlatv2': 'true', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.defaultComputeName': 'NC64asT4v3-GPU', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'id': '/subscriptions/b7d41fc8-d35d-41db-92ed-1f7f1d32d4d9/resourceGroups/monai-3d-rg/providers/Microsoft.MachineLearningServices/workspaces/aml-monai-3d/jobs/sharp_office_l4h4r612dg', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/nc96adsa100-gpu/code/Users/aspiridonov/monai-3D-segmentation/brats-mri-segmentation-segresnet/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f5f62fff880>, 'serialize': <msrest.serialization.Serializer object at 0x7f5f62fffd90>, 'display_name': 'image_pipeline', 'experiment_name': 'monai-brain-tumor-segmentation', 'compute': None, 'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x7f5f62fffd30>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x7f5f62fffe50>}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>monai-brain-tumor-segmentation</td><td>sharp_office_l4h4r612dg</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/sharp_office_l4h4r612dg?wsid=/subscriptions/b7d41fc8-d35d-41db-92ed-1f7f1d32d4d9/resourcegroups/monai-3d-rg/workspaces/aml-monai-3d&amp;tid=43083d15-7273-40c1-b7db-39efd9ccc17a\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680762687378
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}